{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import hydra.experimental\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torchfly.text.decode import TransformerDecoder\n",
    "from torchfly.common import set_random_seed, move_to_device\n",
    "\n",
    "from configure_dataloader import DataLoaderHandler\n",
    "from model import Generator, TextGAILModel\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.experimental.initialize(\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hydra.experimental.compose(\"config.yaml\")\n",
    "print(config.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_handler = DataLoaderHandler(config)\n",
    "valid_dataloader = dataloader_handler.test_dataloader(config)\n",
    "collate_fn = test_dataloader.dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGAILModel(config)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config.decode)\n",
    "decoder.register_generator(model.generator.decoder)\n",
    "decoder.register_tokenizer(tokenizer)\n",
    "decoder.prepare_model_inputs_for_generation = model.generator.prepare_model_inputs_for_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to store results\n",
    "os.makedirs(config.task.name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.mle_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_weights = torch.load(config.task.mle_weights_path)\n",
    "model.generator.load_state_dict(mle_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = (np.arange(10) + 1) / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/mle_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_write = open(f\"{config.task.name}/mle_beam_4.txt\", \"w\")\n",
    "\n",
    "# for batch in tqdm.tqdm(valid_dataloader):\n",
    "#     batch = collate_fn(batch)\n",
    "#     batch = move_to_device(batch, device)\n",
    "\n",
    "#     ground_truth = batch[\"target_text\"]\n",
    "\n",
    "#     results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "#     generated = []\n",
    "\n",
    "#     for i in range(len(results[\"tokens\"])):\n",
    "#         res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "#         generated.append(res)\n",
    "\n",
    "#     for gt, gen in zip(ground_truth, generated):\n",
    "#         f_write.write(json.dumps([gt, gen]))\n",
    "#         f_write.write(\"\\n\")        \n",
    "\n",
    "# f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGAIL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.textgail_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgail_weights = torch.load(config.task.textgail_weights_path)\n",
    "model.load_state_dict(textgail_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/textgail_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_write = open(f\"{config.task.name}/textgail_no_pretrain2_beam_4.txt\", \"w\")\n",
    "\n",
    "for batch in tqdm.tqdm(valid_dataloader):\n",
    "    batch = collate_fn(batch)\n",
    "    batch = move_to_device(batch, device)\n",
    "\n",
    "    ground_truth = batch[\"target_text\"]\n",
    "\n",
    "    results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "    generated = []\n",
    "\n",
    "    for i in range(len(results[\"tokens\"])):\n",
    "        res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "        generated.append(res)\n",
    "        \n",
    "    for gt, gen in zip(ground_truth, generated):\n",
    "        f_write.write(json.dumps([gt, gen]))\n",
    "        f_write.write(\"\\n\")        \n",
    "\n",
    "f_write.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}